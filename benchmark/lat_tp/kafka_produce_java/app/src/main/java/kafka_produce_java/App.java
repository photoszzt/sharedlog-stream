/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package kafka_produce_java;

import java.io.IOException;
import java.io.OutputStream;
import java.net.InetSocketAddress;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.time.Duration;
import java.time.Instant;
import java.time.temporal.ChronoUnit;
import java.time.temporal.TemporalUnit;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Properties;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.Future;
import java.util.concurrent.TimeUnit;

import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.DefaultParser;
import org.apache.commons.cli.Option;
import org.apache.commons.cli.Options;
import org.apache.commons.cli.ParseException;
import org.apache.kafka.clients.admin.Admin;
import org.apache.kafka.clients.admin.AdminClientConfig;
import org.apache.kafka.clients.admin.CreateTopicsResult;
import org.apache.kafka.clients.admin.NewTopic;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;
import org.apache.kafka.common.config.TopicConfig;

import com.sun.net.httpserver.HttpServer;
import com.sun.net.httpserver.HttpHandler;
import com.sun.net.httpserver.HttpExchange;
import org.apache.kafka.common.serialization.Serde;

public class App {
    private static final Option BOOTSTRAP_SERVER = new Option("b", "bootstrap-server", true, "Kafka bootstrap server");
    private static final Option EVENTS_NUM = new Option("ev", "events-num", true, "Number of events to produce");
    private static final Option DURATION = new Option("d", "duration", true, "Duration of the test in seconds");
    private static final Option PAYLOAD_FILE = new Option("p", "payload-file", true, "File containing the payload");
    private static final Option TPS = new Option("t", "tps", true, "Target TPS");

    private static final String TOPIC_NAME = "src";

    public static void main(String[] args) throws ParseException, IOException {
        if (args == null || args.length == 0) {
            System.out.println(
                    "Usage: java -jar kafka-produce-java.jar -b <bootstrap-server> -ev <events-num> -d <duration> -p <payload-file> -t <tps>");
            System.exit(1);
        }
        Options options = getOptions();
        DefaultParser parser = new DefaultParser();
        CommandLine line = parser.parse(options, args, true);
        String bootstrapServer = line.getOptionValue(BOOTSTRAP_SERVER.getOpt());
        String eventsNumStr = line.getOptionValue(EVENTS_NUM.getOpt());
        String durString = line.getOptionValue(DURATION.getOpt());
        String payloadFile = line.getOptionValue(PAYLOAD_FILE.getOpt());
        String tpsStr = line.getOptionValue(TPS.getOpt());
        if ((payloadFile == null) || (payloadFile == "")) {
            System.out.println("Payload file is required");
            System.exit(1);
        }
        Path p = Paths.get(payloadFile);
        if (!p.toFile().exists()) {
            System.out.println("Payload file does not exist");
            System.exit(1);
        }
        byte[] content = Files.readAllBytes(p);
        Duration dur = Duration.ofSeconds(Long.parseLong(durString));
        long durNano = dur.toNanos();
        long tps = Long.parseLong(tpsStr);
        long timeGapUs = 1000000 / tps;
        long events = Long.parseLong(eventsNumStr);

        Admin admin = createAdminClient(bootstrapServer);
        NewTopic topic = new NewTopic(TOPIC_NAME, 1, (short) 3);
        topic = topic.configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, "3"));
        CreateTopicsResult crt = admin.createTopics(Collections.singleton(topic));
        try {
            crt.all().wait();
        } catch (InterruptedException e1) {
            e1.printStackTrace();
        }
        MsgpPOJOSerde<PayloadTs> payloadSerde = new MsgpPOJOSerde<>();
        payloadSerde.setClass(PayloadTs.class);
        KafkaProducer<String, byte[]> k = createKafkaProducer(bootstrapServer);
        final CountDownLatch latch = new CountDownLatch(1);
        Runnable r = () -> {
            Instant start = Instant.now();
            long startNano = System.nanoTime();
            Instant next = Instant.now();
            int idx = 0;
            ArrayList<Future<RecordMetadata>> sent = new ArrayList<>(1024);

            while(true) {
                if ((System.nanoTime() - startNano > durNano) || (events != 0 && idx >= events)) {
                    break;
                }
                next = next.plus(timeGapUs, ChronoUnit.MICROS);
                long ts = ChronoUnit.MICROS.between(Instant.EPOCH, Instant.now());
                PayloadTs payload = new PayloadTs(ts, content);
                byte[] encoded = payloadSerde.serialize(TOPIC_NAME, payload);
                Instant now = Instant.now();
                if (next.isAfter(now)) {
                    Duration toSleep = Duration.between(now, next);
                    long nanoToSleep = toSleep.getNano();
                    try {
                        TimeUnit.NANOSECONDS.sleep(nanoToSleep);
                    } catch (InterruptedException e) {
                        e.printStackTrace();
                    }
                }
                k.send(new ProducerRecord<>(TOPIC_NAME, encoded));
                idx += 1;
            }
            k.flush();
            Duration totalTime = Duration.between(start, Instant.now());
            System.out.println("produce " + idx + " events, time: " + totalTime + ", throughput: " +
                    (double)idx / totalTime.getSeconds());
            latch.countDown();
        };
        Thread t = new Thread(r);
        HttpServer server = HttpServer.create(new InetSocketAddress(8080), 0);
        server.createContext("/produce", new HttpHandler() {
            @Override
            public void handle(HttpExchange exchange) throws IOException {
                System.out.println("Got connection\n");

                t.start();
                System.out.println("Start processing and waiting for result\n");
                try {
                    latch.await();
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
                byte[] response = "OK".getBytes();
                exchange.sendResponseHeaders(200, response.length);
                OutputStream os = exchange.getResponseBody();
                os.write(response);
                os.close();
            }
        });
        server.setExecutor(null);
        server.start();
    }

    private static KafkaProducer<String, byte[]> createKafkaProducer(String boostrapServer) {
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, boostrapServer);
        props.put(ProducerConfig.ACKS_CONFIG, "all");
        props.put(ProducerConfig.BATCH_SIZE_CONFIG, 0);
        props.put(ProducerConfig.LINGER_MS_CONFIG, 0);
        props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 5);
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
                "org.apache.kafka.common.serialization.ByteArraySerializer");
        return new KafkaProducer<>(props);
    }

    private static Admin createAdminClient(String boostrapServer) {
        Properties props = new Properties();
        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, boostrapServer);
        return Admin.create(props);
    }

    private static Options getOptions() {
        Options options = new Options();
        options.addOption(BOOTSTRAP_SERVER);
        options.addOption(EVENTS_NUM);
        options.addOption(DURATION);
        options.addOption(PAYLOAD_FILE);
        options.addOption(TPS);
        return options;
    }
}
