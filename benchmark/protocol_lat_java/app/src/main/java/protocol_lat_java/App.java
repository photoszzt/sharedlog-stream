/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package protocol_lat_java;

import java.io.IOException;
import java.io.OutputStream;
import java.net.InetSocketAddress;
import java.time.Duration;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Properties;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.ExecutionException;

import org.apache.commons.cli.CommandLine;
import org.apache.commons.cli.DefaultParser;
import org.apache.commons.cli.HelpFormatter;
import org.apache.commons.cli.Option;
import org.apache.commons.cli.Options;
import org.apache.commons.cli.ParseException;
import org.apache.kafka.clients.admin.Admin;
import org.apache.kafka.clients.admin.AdminClientConfig;
import org.apache.kafka.clients.admin.CreateTopicsResult;
import org.apache.kafka.clients.admin.NewTopic;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.consumer.OffsetAndMetadata;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.KafkaException;
import org.apache.kafka.common.KafkaFuture;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.config.TopicConfig;

import com.sun.net.httpserver.HttpServer;
import com.sun.net.httpserver.HttpHandler;
import com.sun.net.httpserver.HttpExchange;

public class App {
    private static final String CONSUMER_GROUP_ID = "bench";
    private static final String OUTPUT_TOPIC = "output";
    private static final String INPUT_TOPIC = "src";
    private static final Option BOOTSTRAP_SERVER = new Option("b", "bootstrap-server", true, "Kafka bootstrap server");
    private static final Option PORT = new Option("p", "port", true, "Port to listen on for starting the tests");
    private static final Option DURATION = new Option("dur", "duration", true, "Duration of the test");

    public static void main(String[] args) throws ParseException, IOException {
        Options options = getOptions();
        if (args == null || args.length == 0) {
            HelpFormatter formatter = new HelpFormatter();
            formatter.printHelp("java -jar protocol-lat-java.jar", options);
            System.exit(1);
        }
        DefaultParser parser = new DefaultParser();
        CommandLine line = parser.parse(options, args, true);
        String portStr = line.getOptionValue(PORT.getOpt());
        String bootstrapServer = line.getOptionValue(BOOTSTRAP_SERVER.getOpt());
        String durString = line.getOptionValue(DURATION.getOpt());
        int port = portStr == null ? 8090 : Integer.parseInt(portStr);
        Duration dur = durString == null ? Duration.ofSeconds(80) : Duration.ofSeconds(Integer.parseInt(durString));
        long durNano = dur.toNanos();

        long commitEveryNano = Duration.ofMillis(100).toNanos();

        KafkaConsumer<String, byte[]> consumer = createKafkaConsumer(bootstrapServer);
        KafkaProducer<String, byte[]> producer = createKafkaProducer(bootstrapServer);
        consumer.subscribe(Collections.singleton(INPUT_TOPIC));
        Admin admin = createAdminClient(bootstrapServer);
        NewTopic topic = new NewTopic(OUTPUT_TOPIC, 1, (short) 3);
        topic = topic.configs(Collections.singletonMap(TopicConfig.MIN_IN_SYNC_REPLICAS_CONFIG, "3"));
        CreateTopicsResult crt = admin.createTopics(Collections.singleton(topic));
        KafkaFuture<Void> future = crt.all();
        try {
            future.get();
        } catch (InterruptedException | ExecutionException e1) {
            e1.printStackTrace();
        }
        final CountDownLatch latch = new CountDownLatch(1);
        Thread t = new Thread(() -> {
            producer.initTransactions();
            producer.beginTransaction();
            long commitTimer = System.nanoTime();
            long start = System.nanoTime();
            ArrayList<Long> latencies = new ArrayList<>(1024);
            try {
                Map<TopicPartition, OffsetAndMetadata> offsets = new HashMap<>();
                while (true) {
                    if (System.nanoTime() - commitTimer > commitEveryNano) {
                        long beg = System.nanoTime();
                        producer.sendOffsetsToTransaction(offsets, consumer.groupMetadata());
                        producer.commitTransaction();
                        producer.beginTransaction();
                        long end = System.nanoTime();
                        long elapsedNano = end - beg;
                        latencies.add(elapsedNano);
                        offsets.clear();
                        commitTimer = System.nanoTime();
                    }
                    if (System.nanoTime() - start > durNano) {
                        break;
                    }
                    ConsumerRecords<String, byte[]> records = consumer.poll(Duration.ofMillis(5));
                    for (TopicPartition tp : records.partitions()) {
                        long lastOffset = -1;
                        for (var record : records.records(tp)) {
                            producer.send(new ProducerRecord<>(OUTPUT_TOPIC, record.value()));
                            lastOffset = record.offset();
                        }
                        offsets.put(tp, new OffsetAndMetadata(lastOffset + 1));
                    }
                }
                consumer.close();
                producer.abortTransaction();
                producer.close();
                System.out.println("\nCommitTime: " + latencies + "\n");
                Collections.sort(latencies);
                System.out.println("p50: " + P(latencies, 0.5) +
                        " p90: " + P(latencies, 0.9) + 
                        " p99: " + P(latencies, 0.99));
            } catch (KafkaException e) {
                producer.abortTransaction();
            }
            latch.countDown();
        });
        HttpServer server = HttpServer.create(new InetSocketAddress(port), 0);
        server.createContext("/run", new HttpHandler() {
            @Override
            public void handle(HttpExchange exchange) throws IOException {
                System.out.println("Got connection\n");

                t.start();
                System.out.println("Start processing and waiting for result\n");
                try {
                    latch.await();
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
                byte[] response = "OK".getBytes();
                exchange.sendResponseHeaders(200, response.length);
                OutputStream os = exchange.getResponseBody();
                os.write(response);
                os.close();
            }
        });
        server.setExecutor(null);
        server.start();
    }

    private static long P(ArrayList<Long> arr, double percent) {
        return arr.get((int) (((double) arr.size()) * percent + 0.5) - 1);
    }

    private static KafkaConsumer<String, byte[]> createKafkaConsumer(String boostrapServer) {
        final Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, boostrapServer);
        props.put(ConsumerConfig.ALLOW_AUTO_CREATE_TOPICS_CONFIG, "false");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, CONSUMER_GROUP_ID);
        props.put(ConsumerConfig.ISOLATION_LEVEL_CONFIG, "read_committed");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,
                "org.apache.kafka.common.serialization.StringDeserializer");
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,
                "org.apache.kafka.common.serialization.ByteArrayDeserializer");
        return new KafkaConsumer<>(props);
    }

    private static KafkaProducer<String, byte[]> createKafkaProducer(String boostrapServer) {
        Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, boostrapServer);
        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, "true");
        props.put(ProducerConfig.ACKS_CONFIG, "all");
        props.put(ProducerConfig.BATCH_SIZE_CONFIG, 0);
        props.put(ProducerConfig.LINGER_MS_CONFIG, 0);
        props.put(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, 5);
        props.put(ProducerConfig.TRANSACTIONAL_ID_CONFIG, "bench-prod-1");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringSerializer");
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
                "org.apache.kafka.common.serialization.ByteArraySerializer");
        return new KafkaProducer<>(props);
    }

    private static Admin createAdminClient(String boostrapServer) {
        Properties props = new Properties();
        props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, boostrapServer);
        return Admin.create(props);
    }

    private static Options getOptions() {
        Options options = new Options();
        options.addOption(BOOTSTRAP_SERVER);
        options.addOption(PORT);
        options.addOption(DURATION);
        return options;
    }
}
